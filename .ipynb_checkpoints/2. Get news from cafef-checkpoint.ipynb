{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#internet package\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup  \n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "#functional package\n",
    "import logging\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "logging.basicConfig(filename=\"crawl.log\",level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_new_data(symbol):\n",
    "    r = http.request('GET', 'http://s.cafef.vn/Ajax/Events_RelatedNews_New.aspx?symbol=' + str(symbol) + '&floorID=0&configID=0&PageIndex=1&PageSize=10000&Type=2')\n",
    "    soup = BeautifulSoup(r.data, \"html.parser\")\n",
    "    data = soup.find(\"ul\", {\"class\": \"News_Title_Link\"})\n",
    "    raw = data.find_all('li')\n",
    "    data_dicts = []\n",
    "    for row in raw:\n",
    "        row_dict = {}\n",
    "        row_dict['newsdate'] = row.span.text\n",
    "        row_dict['title'] = row.a.text\n",
    "        row_dict['url']  = \"http://s.cafef.vn/\" + str(row.a['href'])\n",
    "        row_dict['ticker'] = str(symbol)\n",
    "        data_dicts.append(row_dict)\n",
    "    return data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(get_new_data(\"VNM\")).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_company_list = pd.read_csv(\"public_company_list.csv\", skiprows = 1, usecols = range(1,5))\n",
    "public_company_list = public_company_list[public_company_list[\"SÀN\"] != \"OTC\"]\n",
    "public_company_list = public_company_list.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A32', 'AAA', 'AAM', ..., 'YRC', 'YSC', 'YTC'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_company_list['MÃ CK'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1711/1711 [26:19<00:00,  2.64it/s] \n"
     ]
    }
   ],
   "source": [
    "all_datas = pd.DataFrame()\n",
    "for ticker in tqdm(public_company_list['MÃ CK'].values):\n",
    "\n",
    "    tickernews = pd.DataFrame(get_new_data(ticker))\n",
    "    all_datas = pd.concat([all_datas, tickernews], ignore_index=True)\n",
    "\n",
    "# to export data only \n",
    "# all_datas.to_csv(\"news_url.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of Empty DataFrame\n",
       "Columns: []\n",
       "Index: []>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datas.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_url = pd.read_csv(\"news_url.csv\", usecols = range(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://s.cafef.vn//a32-314278/a32-ong-nguyen-the-anh-pho-chu-tich-hdqt-da-mua-2800-cp.chn',\n",
       "       'http://s.cafef.vn//a32-312903/a32-ong-nguyen-the-anh-pho-chu-tich-hdqt-dang-ky-mua-2800-cp.chn',\n",
       "       'http://s.cafef.vn//a32-309371/diem-danh-nhung-doanh-nghiep-chot-quyen-nhan-co-tuc-bang-tien-bang-co-phieu-va-co-phieu-thuong-tuan-nay.chn',\n",
       "       'http://s.cafef.vn//a32-308683/lich-chot-quyen-nhan-co-tuc-bang-tien-cua-8-doanh-nghiep.chn',\n",
       "       'http://s.cafef.vn//a32-308769/a32-662019-ngay-gdkhq-tra-co-tuc-bang-tien-mat-700dcp.chn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_url.head()['url'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsdate</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/07/2019 09:34</td>\n",
       "      <td>A32</td>\n",
       "      <td>A32: Ông Nguyễn Thế Anh - Phó Chủ tịch HĐQT đã...</td>\n",
       "      <td>http://s.cafef.vn//a32-314278/a32-ong-nguyen-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/07/2019 14:21</td>\n",
       "      <td>A32</td>\n",
       "      <td>A32: Ông Nguyễn Thế Anh - Phó Chủ tịch HĐQT đă...</td>\n",
       "      <td>http://s.cafef.vn//a32-312903/a32-ong-nguyen-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/06/2019 08:55</td>\n",
       "      <td>A32</td>\n",
       "      <td>Điểm danh những doanh nghiệp chốt quyền nhận c...</td>\n",
       "      <td>http://s.cafef.vn//a32-309371/diem-danh-nhung-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28/05/2019 13:46</td>\n",
       "      <td>A32</td>\n",
       "      <td>Lịch chốt quyền nhận cổ tức bằng tiền của 8 do...</td>\n",
       "      <td>http://s.cafef.vn//a32-308683/lich-chot-quyen-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28/05/2019 13:27</td>\n",
       "      <td>A32</td>\n",
       "      <td>A32: 6.6.2019, ngày GDKHQ trả cổ tức bằng tiền...</td>\n",
       "      <td>http://s.cafef.vn//a32-308769/a32-662019-ngay-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           newsdate ticker                                              title  \\\n",
       "0  10/07/2019 09:34    A32  A32: Ông Nguyễn Thế Anh - Phó Chủ tịch HĐQT đã...   \n",
       "1  01/07/2019 14:21    A32  A32: Ông Nguyễn Thế Anh - Phó Chủ tịch HĐQT đă...   \n",
       "2  03/06/2019 08:55    A32  Điểm danh những doanh nghiệp chốt quyền nhận c...   \n",
       "3  28/05/2019 13:46    A32  Lịch chốt quyền nhận cổ tức bằng tiền của 8 do...   \n",
       "4  28/05/2019 13:27    A32  A32: 6.6.2019, ngày GDKHQ trả cổ tức bằng tiền...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://s.cafef.vn//a32-314278/a32-ong-nguyen-t...  \n",
       "1  http://s.cafef.vn//a32-312903/a32-ong-nguyen-t...  \n",
       "2  http://s.cafef.vn//a32-309371/diem-danh-nhung-...  \n",
       "3  http://s.cafef.vn//a32-308683/lich-chot-quyen-...  \n",
       "4  http://s.cafef.vn//a32-308769/a32-662019-ngay-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 330851 entries, 0 to 330850\n",
      "Data columns (total 4 columns):\n",
      "newsdate    330851 non-null object\n",
      "ticker      330851 non-null object\n",
      "title       330849 non-null object\n",
      "url         330851 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "news_url.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://s.cafef.vn//a32-308769/a32-662019-ngay-gdkhq-tra-co-tuc-bang-tien-mat-700dcp.chn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_details_from_url(df_to_get):\n",
    "    data_dicts = []\n",
    "    for newsdate, url in tqdm_notebook(zip(df_to_get['newsdate'].values, df_to_get['url'].values)):\n",
    "        row_dict = {}\n",
    "        try:\n",
    "            article = Article(url, language='vi')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            row_dict['newsdate'] = newsdate\n",
    "            row_dict['summary'] = article.summary\n",
    "            row_dict['keywords'] = article.keywords\n",
    "            data_dicts.append(row_dict)\n",
    "        except Exception as e:\n",
    "#             logging.error()\n",
    "            continue\n",
    "#         print(data_dicts)\n",
    "    return data_dicts\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b7dd6b8dd346aabaa93b98e4a5c3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8fbe945f3f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_detail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_details_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ad9f24a76965>\u001b[0m in \u001b[0;36mget_details_from_url\u001b[0;34m(df_to_get)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mrow_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newsdate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewsdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Before any computations on the body, clean DOM object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_best_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/newspaper/cleaners.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(self, doc_to_clean)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_drop_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_scripts_styles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_bad_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_nodes_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdoc_to_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_nodes_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_to_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/newspaper/cleaners.py\u001b[0m in \u001b[0;36mclean_bad_tags\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclean_bad_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mnaughty_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnauthy_ids_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnaughty_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_article\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/newspaper/parsers.py\u001b[0m in \u001b[0;36mxpath_re\u001b[0;34m(cls, node, expression)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpath_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mregexp_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://exslt.org/regular-expressions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m're'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregexp_namespace\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._Element.xpath\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree.XPathElementEvaluator.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree._XPathEvaluatorBase._handle_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/extensions.pxi\u001b[0m in \u001b[0;36mlxml.etree._extension_function_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/extensions.pxi\u001b[0m in \u001b[0;36mlxml.etree._unwrapXPathObject\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/extensions.pxi\u001b[0m in \u001b[0;36mlxml.etree._elementStringResultFactory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "news_detail = pd.DataFrame(get_details_from_url(news_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the news with full text and summarize and keywords. Run this one only if you want to see what happended, actually the script need about 24h~ to craw data from cafef. You can get the \"newfull.csv\" file in github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#internet package\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup  \n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "#functional package\n",
    "import logging\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "logging.basicConfig(filename=\"crawlfulltext.log\",level=logging.ERROR)\n",
    "\n",
    "news_url = pd.read_csv(\"news_url.csv\", usecols = range(1,5))\n",
    "\n",
    "\n",
    "def get_details_from_url(df_to_get):\n",
    "    data_dicts = []\n",
    "    for newsdate, url in tqdm(zip(df_to_get['newsdate'].values, df_to_get['url'].values)):\n",
    "        row_dict = {}\n",
    "        try:\n",
    "            article = Article(url, language='vi')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            row_dict['newsdate'] = newsdate\n",
    "            row_dict['fulltext'] = article.text\n",
    "            row_dict['summnary'] = article.summary\n",
    "            row_dict['keyword'] = article.keywords\n",
    "            data_dicts.append(row_dict)\n",
    "        except Exception:\n",
    "#             logging.error()\n",
    "            continue\n",
    "    return data_dicts\n",
    "\n",
    "\n",
    "# from multiprocessing.dummy import Pool as ThreadPool \n",
    "# pool = ThreadPool(32)\n",
    "# results = pool.map(get_details_from_url(news_url))\n",
    "# pool.close() \n",
    "# pool.join()\n",
    "\n",
    "newfull = pd.DataFrame(get_details_from_url(news_url))\n",
    "newfull.to_csv('newfull.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
